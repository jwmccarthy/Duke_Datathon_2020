---
title: "Datathon_Unemployment"
author: "Rob Kravec"
date: "10/31/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ipumsr)
library(tidyverse)
library(rvest)
library(zoo)
```

```{r load-data}
ddi <- read_ipums_ddi("cps_00002.xml")
data <- read_ipums_micro(ddi)
```

Start with data cleaning

```{r data-cleaning}
# Start by looking at the most recent data, which we have at a monthly level
# Filter for ages 21-65
data_clean <- data %>% 
  filter(YEAR >= 2019,
         AGE >= 21,
         AGE < 65,
         LABFORCE > 0)

# Check for incidence of missing employment status flag
mean(data_clean$EMPSTAT == 0)
mean(data_clean$LABFORCE == 0)

# Create relevant variables for unemployment rate calculations
data_clean <- data_clean %>% 
  mutate(employed = as.numeric(EMPSTAT %in% c(1, 10, 12)) * WTFINL,
         lf_weighted = (LABFORCE - 1) * WTFINL)
```

Create a data frame grouped by relevant variables to determine unemployment rate

```{r group-df}
# Create grouped data frame
data_clean_group <- data_clean %>% 
  group_by(YEAR, MONTH, STATEFIP, METFIPS) %>% 
  summarise(employed = sum(employed),
            labor_force = sum(lf_weighted),
            total_pop = sum(WTFINL))

# Calculate unemployment rate
data_clean_group <- data_clean_group %>% 
  mutate(unemp_rt = 1 - employed / labor_force)

# Sanity check: How many employed people are there at a point in time?
jan_2019 <- data_clean_group %>% 
  filter(YEAR == 2019, MONTH == 1)
sum(jan_2019$employed)
# We get 138M, which seems about right (depending on where you look on the 
# internet)

# Another sanity check: What is the overall unemployment rate at a point in time?
1 - sum(jan_2019$employed) / sum(jan_2019$labor_force)
# 4.0% -- that also looks right!
```

At this point, if we're interested in metro-area-level trends, we need to 
exclude observations with unobserved METFIPS values

```{r filter}
# Check for percentage of labor force that pertains to an unidentified 
# METFIPS value
sum(data_clean_group[which(data_clean_group$METFIPS %in% c(99998, 99999)),]$labor_force) / sum(data_clean_group$labor_force)
# That's not great, but it looks like we'll lose observations pertaining to 
# about 15% of the labor force

# Create filtered df. Cgf = cleaned, grouped, filtered
data_cgf <- data_clean_group %>% 
  filter(!(METFIPS %in% c(99998, 99999)))
```

Scrape METFIPS crosswalk from ipums website

```{r metfips-scrape}
# Define URL
url <- "https://cps.ipums.org/cps/codes/metfips_2014onward_codes.shtml"

# Pull the location
fips_location <- read_html(url) %>% 
  html_nodes(css = "dd") %>% 
  html_text()

# Pull the code
fips_code <- read_html(url) %>% 
  html_nodes(css = "dt") %>% 
  html_text()

# Combine location and code into a crosswalk
fips_cw <- data.frame(METFIPS = as.numeric(fips_code), 
                      location = fips_location)
```

Add location label to data frame with unemployment data

```{r merge-location}
# Join data sets
data_cgf_loc <- left_join(x = data_cgf, y = fips_cw, by = "METFIPS")

# Create a single date column
data_cgf_loc <- data_cgf_loc %>% 
  mutate(year_mon = as.yearmon(paste(YEAR, MONTH), "%Y %m"))
```

Let's do a quick visualization of a few cities just to make sure everything 
looks OK

```{r plot-unemployment-rate}
# Tucson, AZ
data_cgf_loc %>% 
  filter(location == "Tucson, AZ") %>% 
  ggplot(mapping = aes(x = year_mon, y = unemp_rt)) + geom_line() +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Tucson, AZ") +
  theme(plot.title = element_text(hjust = 0.5))

# Birmingham, AL
data_cgf_loc %>% 
  filter(location == "Birmingham-Hoover, AL") %>% 
  ggplot(mapping = aes(x = year_mon, y = unemp_rt)) + geom_line() +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Birmingham, AL") +
  theme(plot.title = element_text(hjust = 0.5))

# Fresno, CA
data_cgf_loc %>% 
  filter(location == "Fresno, CA") %>% 
  ggplot(mapping = aes(x = year_mon, y = unemp_rt)) + geom_line() +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Fresno, CA") +
  theme(plot.title = element_text(hjust = 0.5))
```

Create some time series variables

```{r}
# Focus on three month and six month rolling average
data_cgf_loc <- data_cgf_loc %>% 
  ungroup() %>% 
  group_by(location) %>% 
  arrange(location, YEAR, MONTH) %>% 
  mutate(lag1 = lag(x = unemp_rt, n = 1),
         lag2 = lag(x = unemp_rt, n = 2),
         lag3 = lag(x = unemp_rt, n = 3),
         lag4 = lag(x = unemp_rt, n = 4),
         lag5 = lag(x = unemp_rt, n = 5),
         avg3 = (unemp_rt + lag1 + lag2) / 3,
         avg6 = (unemp_rt + lag1 + lag2 + lag3 + lag4 + lag5) / 6
         )
```

Let's see if the trends are a little smoother for `avg3` and `avg6` in the 3
cities that we visualized before

```{r plot-unemployment-rate-smooth}
# Tucson, AZ
data_cgf_loc %>% 
  filter(location == "Tucson, AZ") %>% 
  ggplot(mapping = aes(x = year_mon)) + 
  geom_line(mapping = aes(y = unemp_rt, color = "Point in time")) +
  geom_line(mapping = aes(y = avg3, color = "3 month")) +
  geom_line(mapping = aes(y = avg6, color = "6 month")) +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Tucson, AZ",
       color = "Legend") +
  theme(plot.title = element_text(hjust = 0.5))

# Birmingham, AL
data_cgf_loc %>% 
  filter(location == "Birmingham-Hoover, AL") %>% 
  ggplot(mapping = aes(x = year_mon)) + 
  geom_line(mapping = aes(y = unemp_rt, color = "Point in time")) +
  geom_line(mapping = aes(y = avg3, color = "3 month")) +
  geom_line(mapping = aes(y = avg6, color = "6 month")) +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Birmingham, AL",
       color = "Legend") +
  theme(plot.title = element_text(hjust = 0.5))

# Fresno, CA
data_cgf_loc %>% 
  filter(location == "Fresno, CA") %>% 
  ggplot(mapping = aes(x = year_mon)) + 
  geom_line(mapping = aes(y = unemp_rt, color = "Point in time")) +
  geom_line(mapping = aes(y = avg3, color = "3 month")) +
  geom_line(mapping = aes(y = avg6, color = "6 month")) +
  labs(x = "Date", y = "Unemployment rate",
       title = "Unemployment rate over time in Fresno, CA",
       color = "Legend") +
  theme(plot.title = element_text(hjust = 0.5))
```

Create variables that indicate changes in rolling average unemployment rate

```{r diff-variables}
data_cgf_loc <- data_cgf_loc %>% 
  mutate(diff1 = avg3 - lag(x = avg3, n = 1),
         diff3 = avg3 - lag(x = avg3, n = 3),
         diff6 = avg3 - lag(x = avg3, n = 6),
         diff12 = avg3 - lag(x = avg3, n = 12)
         )
```

Flag worst unemployment rate month for each location

```{r worst-unemp-rt}
data_cgf_loc <- data_cgf_loc %>% 
  mutate(worst_month = ifelse(avg3 == max(avg3, na.rm = T), 1, 0))
```

NOTE: I've noticed a non-negligible number of entries observations with a 3-month
rolling average unemployment rate of 0. That's certainly not ideal, and we may
need to do some filtering going forward to remove those locations

```{r flaggin-zeros}
# Inspect locations that achieve a 3-month rolling average of zero unemployment
zeros <- data_cgf_loc %>% 
  filter(avg3 == 0)
# While some of these may make sense (like heavy tourist towns), most are likely
# a result of poor data collection

# We'll create a flag for these locations and potentially exclude later on
data_cgf_loc <- data_cgf_loc %>% 
  mutate(zero_unemp = ifelse(location %in% zeros$location, 1, 0))
```

At the peak of unemployment rates (for each location), which locations had the
highest absolute unemployment rates? Highest increase in unemployment (over 
different time horizons)?

```{r largest-unemp}
# Highest unemployment at peak
data_cgf_loc %>% 
  filter(worst_month == 1, YEAR == 2020, !is.na(avg6), zero_unemp == 0) %>% 
  select(MONTH, location, unemp_rt, avg3, avg6, diff1, diff3, diff6, diff12) %>% 
  arrange(desc(unemp_rt))

# Highest 3 month increase in unemployment at peak
data_cgf_loc %>% 
  filter(worst_month == 1, YEAR == 2020, !is.na(avg6), zero_unemp == 0) %>% 
  select(MONTH, location, unemp_rt, avg3, avg6, diff1, diff3, diff6, diff12) %>% 
  arrange(desc(diff3))

# Highest 12 month increase in unemployment at peak
data_cgf_loc %>% 
  filter(worst_month == 1, YEAR == 2020, !is.na(avg6), zero_unemp == 0) %>% 
  select(MONTH, location, unemp_rt, avg3, avg6, diff1, diff3, diff6, diff12) %>% 
  arrange(desc(diff12))
```

Looking at September 2020 data, which locations are not doing significantly 
worse (or even better) on unemployment rate, relative to 12 months prior?

```{r}
data_cgf_loc %>% 
  filter(YEAR == 2020, MONTH == 9, zero_unemp == 0) %>% 
  select(MONTH, location, unemp_rt, avg3, avg6, diff1, diff3, diff6, diff12) %>% 
  arrange(diff12)
```

Save dataset to data folder that is easily accessible

```{r save-data}
saveRDS(object = data_cgf_loc, file = "data/unemployment.RDS")
```

It turns out that we need to have county FIP code in order to merge with a
different data source about COVID rates. I'll now work on getting a version
of the data that is grouped by county FIPS

```{r}
# Create grouped data frame
data_clean_county <- data_clean %>% 
  filter(COUNTY != 0) %>% 
  group_by(YEAR, MONTH, STATEFIP, COUNTY, METFIPS) %>% 
  summarise(employed = sum(employed),
            labor_force = sum(lf_weighted),
            total_pop = sum(WTFINL))

# Calculate unemployment rate
data_clean_county <- data_clean_county %>% 
  mutate(unemp_rt = 1 - employed / labor_force)

# Add columns
data_clean_county <- data_clean_county %>% 
  ungroup() %>% 
  group_by(COUNTY) %>% 
  arrange(COUNTY, YEAR, MONTH) %>% 
  mutate(lag1 = lag(x = unemp_rt, n = 1),
         lag2 = lag(x = unemp_rt, n = 2),
         lag3 = lag(x = unemp_rt, n = 3),
         lag4 = lag(x = unemp_rt, n = 4),
         lag5 = lag(x = unemp_rt, n = 5),
         avg3 = (unemp_rt + lag1 + lag2) / 3,
         avg6 = (unemp_rt + lag1 + lag2 + lag3 + lag4 + lag5) / 6,
         diff1 = avg3 - lag(x = avg3, n = 1),
         diff3 = avg3 - lag(x = avg3, n = 3),
         diff6 = avg3 - lag(x = avg3, n = 6),
         diff12 = avg3 - lag(x = avg3, n = 12),
         worst_month = ifelse(avg3 == max(avg3, na.rm = T), 1, 0)
         )

# Inspect locations that achieve a 3-month rolling average of zero unemployment
zeros <- data_clean_county %>% 
  filter(avg3 == 0)
# While some of these may make sense (like heavy tourist towns), most are likely
# a result of poor data collection

# We'll create a flag for these locations and potentially exclude later on
data_clean_county <- data_clean_county %>% 
  mutate(zero_unemp = ifelse(COUNTY %in% zeros$COUNTY, 1, 0))

# Add location
# Join data sets
data_clean_county <- left_join(x = data_clean_county, y = fips_cw, by = "METFIPS")

# Create a single date column
data_clean_county <- data_clean_county %>% 
  mutate(year_mon = as.yearmon(paste(YEAR, MONTH), "%Y %m"))

# Save data
saveRDS(object = data_clean_county, file = "data/unemployment_county.RDS")
```

Export merged unemployment and COVID data

```{r}
# Group COVID data
covid_group <- covid_data %>% 
  group_by(county_name, countyFIPS, stateFIPS, year, month) %>% 
  summarise(pop = mean(population),
            cases = max(cases),
            death = max(deaths)) %>% 
  mutate(COUNTY = countyFIPS,
         MONTH = month)

# Combine COVID and unemployment data. Filter for only 2020
unemp_covid <- left_join(x = data_clean_county, y = covid_group, 
                         by = c("COUNTY", "MONTH")) %>% 
  filter(YEAR == 2020)
```

